{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f0f6bf6-20bc-406c-ba44-2f81005b0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a1cf602-a394-43e8-b792-ffc83826d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fdc2712-63e3-49d5-bbc6-a5429bccbe6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ddba5c8acb0ba08a\n",
      "Found cached dataset csv (/home/hsong1101/.cache/huggingface/datasets/csv/default-ddba5c8acb0ba08a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Content', 'Summary'],\n",
       "        num_rows: 696389\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Content', 'Summary'],\n",
       "        num_rows: 174098\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "df = load_dataset('csv', data_files='data/data.csv', split='train')\n",
    "\n",
    "dataset = df.train_test_split(test_size=.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0100b67a-6bbc-41d8-b1b1-a92844e2d8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 696389\n",
      "Test dataset size: 174098\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49deec16-9e5b-482c-a1c9-f323bdaedd37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: \n",
      "By . Associated Press . Last updated at 5:14 AM on 2nd November 2011 . A Florida man pleaded not guilty today to hacking into the emails of stars such as Christina Aguilera, Mila Kunis and Scarlett Johansson, whose nude photos were eventually splashed on the Internet. Christopher Chaney, 35, of Jacksonville, Fla., made his first court appearance in California, where he's been indicted on 26 counts, including unauthorized access to a computer and wiretapping. Invaded: Naked pictures of Scarlett Johansson circulated the internet after they were stolen from her phone . If convicted, he faces up to 121 years in prison. U.S. Magistrate Judge Patrick Walsh denied federal prosecutors' request to remand Chaney to custody but modified his bond to $110,000, and he will wear an electronic monitoring device upon his return to Florida. A trial has been scheduled for Dec. 27. Chaney was arrested as part of a yearlong investigation of celebrity hacking that authorities dubbed 'Operation Hackerazzi.' There were more than 50 victims in the case. Some nude photos taken by Johansson herself were posted on the Internet. Chaney offered some material to celebrity blog sites but there wasn't any evidence that he profited from his scheme, authorities said. Chaney is accused of mining through publicly available data to figure out password and security questions for celebrity accounts. Chaney hacked Google, Apple and Yahoo . email accounts beginning last November through February, then hijacked a forwarding feature so that a copy of every email a celebrity received was sent to an account he controlled, according to court documents. He allegedly used the hacker names . 'trainreqsuckswhat,' ''anonygrrl' and 'jaxjaguars911,' and also used the . victims' identities to illegally access and control computers. A search warrant unsealed and obtained by The Associated Press said Chaney's computer hard drive contained numerous private celebrity photos as well as a document that compiled their extensive personal data. Suspect: Christopher Chaney, centre, is accused of mining through publicly available data to hack into the email accounts of more than 50 celebrities like Scarlett Johansson, Christina Aguilera and Mila Kunis . In arguing for a higher bond and time . behind bars, Assistant U.S. Attorney Lisa Feldman said that even after . FBI agents seized the defendant's computer in February, he continued his . hacking scheme against another actress for six months. She declined to reveal the celebrity's name. 'We have great concern that he can't stop himself,' Feldman said. 'We think detention would.' Chaney said he managed to hack into Johansson's email account to send one of her acquaintances an email containing a nude photo of her in exchange for a photo, authorities said. Collection: A search warrant said Christopher Chaney, who appeared in court today, had numerous private celebrity photos as well as a document that compiled their extensive personal data stored on his hard drive . Johansson told Vanity Fair for its December issue that the photos were meant for Ryan Reynolds, who is now her ex-husband. 'There's nothing wrong with that. It's not like I was shooting a porno,' the actress told the magazine. The pair had their divorce finalized by a judge in July. Chaney has apologized for his actions. His attorney, Christopher Chestnut, told AP that his client doesn't want the case to drag on, but the resolution has to be within reason. 'I think he has a crystal clear view of what is at stake,' Chestnut said during Tuesday's hearing. Celebrities who agreed to be named in the . indictment included Mila Kunis, Christina Aguilera and lesser known . actress Renee Olstead, who has a role on TV show The Secret Life of the . American Teenager . The warrant also said Chaney may have stalked a Connecticut woman online for the past 12 years. The document contends there is probable cause that Chaney violated federal charges of stalking and unauthorized access to a computer. Federal prosecutors also said a second woman has stepped forward and made similar accusations that Chaney stalked her online. They declined to comment further. Chaney told the AP the new allegations are completely false. 'I can't accept responsibility for things I didn't do,' Chaney said. Chestnut said the new allegations amount to nothing more than a publicity stunt designed to damage his client's reputation. Mr  Chestnut said: 'The amount of time, money and energy the authorities have spent pursuing a man who didn't sell anything or profit in any way from this alleged activity is truly remarkable, given everything we are going through in this country.' No other charges have been filed against Chaney, who has a 1998 mail fraud conviction in Florida. He was sentenced to four years' probation. Behind the hack: The FBI showed how the celebrities, more 50 in total and many still anonymous, were hacked in six steps .\n",
      "---------------\n",
      "Summary: \n",
      "Christopher Chaney, 35, charged with hacking into the emails of more than 50 celebs, including Scarlett Johansson, Mila Kunis and Christina Aguilera .\n",
      "Scarlett Johansson admits she took the nude photos herself for then-husband Ryan Reynolds .\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "\n",
    "sample = dataset['train'][randrange(len(dataset[\"train\"]))]\n",
    "print(f\"Content: \\n{sample['Content']}\\n---------------\")\n",
    "print(f\"Summary: \\n{sample['Summary']}\\n---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7db86a99-5156-4204-98e4-e03fca493b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id=\"google/flan-t5-base\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff86c910-274a-4f38-b854-bc4482832c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3f69867dce45488e132d3860c4d18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 512\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"Content\"], truncation=True), batched=True, remove_columns=[\"Content\", \"Summary\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"Summary\"], truncation=True), batched=True, remove_columns=[\"Content\", \"Summary\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7e32648-e14d-4656-8849-3da7e9130143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c0caecfbc14109babcaa45e40d73b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa870cd98cf24c91bff56900b29f3126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    inputs = [item for item in sample[\"Content\"]]\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"Summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"Content\", \"Summary\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "813dc4ca-89f4-44d0-95b3-143dc0a83803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0bced16b4e4e9792e535d0de9ef889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfbdd27508e4df08ec83fa8b0fbfcc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f5e0b56bdf4e3c952b44701365032a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset.push_to_hub('hsong1101/news_summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b028086-6eae-406c-b0bd-2d8eff795e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hsong1101/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9f9f46b-9543-4563-94b7-4ecb20f28d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id=\"google/flan-t5-base\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3713ec0-0931-4fb7-a3f4-8d148d1c3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eef39077-0ae7-429e-a52e-b7946369ea83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google/flan-t5-base'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86c0ee4f-6d50-4206-92e0-d812e3a7fec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flan_t5_summarization'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repository_id = \"flan_t5_summarization\"\n",
    "repository_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "222f1add-db80-4619-93c5-5ebcd13aa6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, delete_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9b148b3-bae0-4c35-b05b-05cf6dcf1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_repo(repo_id=repository_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "567daedb-778d-4609-af15-85768cd6c251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flan_t5_summarization'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repository_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79b5d4-eb21-469e-8800-4fb009081cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    overwrite_output_dir=repository_id,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a73495-146c-40e1-b8f0-2a206aa56f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42feaf-5708-4413-a828-8b0cdc8aa0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model/flan_t5_summarizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e039072-fdad-4780-8bc9-06a8bb0fddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state('model/flan_t5_summarizer_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f83f0-c558-45d1-9084-afbd2d6a72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_pretrained(repository_id)\n",
    "# trainer.create_model_card()\n",
    "# # Push the results to the hub\n",
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b7c58-1a78-4fb1-aaaa-5d4577ded1ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e0783-2e48-4c05-a96e-ebe90b0ea291",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = tokenizer('this is just a test', return_tensors='pt').input_ids.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e907b2-1e1b-4f46-a8da-5c62b393c293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = model.generate(sample_ids)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flan T5",
   "language": "python",
   "name": "flan_t5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
